{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b16ec0d",
   "metadata": {
    "id": "8b16ec0d"
   },
   "source": [
    "# ML Course 2023 |  Sentiment Analysis in Twitter Challenge\n",
    "You can check the updated leaderboard in this [link](https://nimble-hellebore-184.notion.site/ML-Course-2023-Sentiment-Analysis-in-Twitter-Challenge-966b041e7aec4f2eabbc8dc33d64b871)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "EJfbR5fCE1eN",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T14:15:09.433288091Z",
     "start_time": "2023-06-09T14:15:03.725469909Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39634,
     "status": "ok",
     "timestamp": 1682427886056,
     "user": {
      "displayName": "Isabel Valera",
      "userId": "14662300919014123166"
     },
     "user_tz": -120
    },
    "id": "EJfbR5fCE1eN",
    "outputId": "72a2c4bb-195c-4b19-f8e2-8e4a0a49925a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tueplots==0.0.5\n",
      "  Using cached tueplots-0.0.5-py3-none-any.whl (18 kB)\n",
      "Collecting matplotlib (from tueplots==0.0.5)\n",
      "  Using cached matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "Collecting numpy (from tueplots==0.0.5)\n",
      "  Using cached numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached fonttools-4.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots==0.0.5) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Collecting pyparsing<3.1,>=2.3.1 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots==0.0.5) (2.8.2)\n",
      "Collecting importlib-resources>=3.2.0 (from matplotlib->tueplots==0.0.5)\n",
      "  Using cached importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->tueplots==0.0.5) (3.16.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->tueplots==0.0.5) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, numpy, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib, tueplots\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.41.0 importlib-resources-6.0.0 kiwisolver-1.4.4 matplotlib-3.7.2 numpy-1.24.4 pillow-10.0.0 pyparsing-3.0.9 tueplots-0.0.5\n",
      "Collecting sentence-transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence-transformers==2.2.2)\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk (from sentence-transformers==2.2.2)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from sentence-transformers==2.2.2) (1.24.4)\n",
      "Collecting scikit-learn (from sentence-transformers==2.2.2)\n",
      "  Downloading scikit_learn-1.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from sentence-transformers==2.2.2)\n",
      "  Using cached scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting sentencepiece (from sentence-transformers==2.2.2)\n",
      "  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting torch>=1.6.0 (from sentence-transformers==2.2.2)\n",
      "  Using cached torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "Collecting torchvision (from sentence-transformers==2.2.2)\n",
      "  Using cached torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "Collecting tqdm (from sentence-transformers==2.2.2)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2)\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting fsspec (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.1)\n",
      "Collecting sympy (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers==2.2.2) (44.0.0)\n",
      "Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting cmake (from triton==2.0.0->torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
      "  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2)\n",
      "  Downloading safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from nltk->sentence-transformers==2.2.2)\n",
      "  Downloading click-8.1.5-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib (from nltk->sentence-transformers==2.2.2)\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->sentence-transformers==2.2.2)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.8/site-packages (from torchvision->sentence-transformers==2.2.2) (10.0.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Downloading MarkupSafe-2.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2)\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence-transformers==2.2.2)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=47e24b6d4879891b2269bddbc8482fb151426cd0211c2edb03b606f2adf8edbb\n",
      "  Stored in directory: /home/molotkova_s/.cache/pip/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\n",
      "Successfully built lit\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, mpmath, lit, cmake, wheel, urllib3, tqdm, threadpoolctl, sympy, scipy, regex, pyyaml, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, networkx, MarkupSafe, joblib, idna, fsspec, filelock, click, charset-normalizer, certifi, scikit-learn, requests, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nltk, jinja2, nvidia-cusolver-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, triton, torch, torchvision, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.3 certifi-2023.5.7 charset-normalizer-3.2.0 click-8.1.5 cmake-3.26.4 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 jinja2-3.1.2 joblib-1.3.1 lit-16.0.6 mpmath-1.3.0 networkx-3.1 nltk-3.8.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pyyaml-6.0 regex-2023.6.3 requests-2.31.0 safetensors-0.3.1 scikit-learn-1.3.0 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.13.3 torch-2.0.1 torchvision-0.15.2 tqdm-4.65.0 transformers-4.30.2 triton-2.0.0 urllib3-2.0.3 wheel-0.40.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow)\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.56.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0 (from tensorflow)\n",
      "  Downloading h5py-3.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1 (from tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow)\n",
      "  Using cached numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from tensorflow) (44.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.32.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m573.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (6.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow) (3.16.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, pyasn1-modules, opt-einsum, markdown, h5py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.3\n",
      "    Uninstalling urllib3-2.0.3:\n",
      "      Successfully uninstalled urllib3-2.0.3\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 h5py-3.9.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.16 werkzeug-2.3.6 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tueplots==0.0.5\n",
    "!pip3 install sentence-transformers==2.2.2\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61eeb706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.venv/lib/python3.8/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wordcloud in ./.venv/lib/python3.8/site-packages (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in ./.venv/lib/python3.8/site-packages (from wordcloud) (1.24.3)\n",
      "Requirement already satisfied: pillow in ./.venv/lib/python3.8/site-packages (from wordcloud) (10.0.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->wordcloud) (6.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.16.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in ./.venv/lib/python3.8/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib) (6.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in ./.venv/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in ./.venv/lib/python3.8/site-packages (from seaborn) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.25 in ./.venv/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in ./.venv/lib/python3.8/site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (6.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.1->seaborn) (3.16.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tueplots in ./.venv/lib/python3.8/site-packages (0.0.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.8/site-packages (from tueplots) (3.7.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from tueplots) (1.24.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (4.41.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv/lib/python3.8/site-packages (from matplotlib->tueplots) (6.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.venv/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->tueplots) (3.16.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->tueplots) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence_transformers in ./.venv/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (0.16.4)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (3.8.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (2.0.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (0.15.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (4.65.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.venv/lib/python3.8/site-packages (from sentence_transformers) (4.30.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./.venv/lib/python3.8/site-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (44.0.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.6.0->sentence_transformers) (0.40.0)\n",
      "Requirement already satisfied: cmake in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.26.4)\n",
      "Requirement already satisfied: lit in ./.venv/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./.venv/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.venv/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.8/site-packages (from nltk->sentence_transformers) (8.1.5)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.8/site-packages (from nltk->sentence_transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.8/site-packages (from torchvision->sentence_transformers) (10.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.8/site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.8/site-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jupyter in ./.venv/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: notebook in ./.venv/lib/python3.8/site-packages (from jupyter) (6.5.4)\n",
      "Requirement already satisfied: qtconsole in ./.venv/lib/python3.8/site-packages (from jupyter) (5.4.3)\n",
      "Requirement already satisfied: jupyter-console in ./.venv/lib/python3.8/site-packages (from jupyter) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in ./.venv/lib/python3.8/site-packages (from jupyter) (7.6.0)\n",
      "Requirement already satisfied: ipykernel in ./.venv/lib/python3.8/site-packages (from jupyter) (6.24.0)\n",
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.8/site-packages (from jupyter) (8.0.7)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (8.12.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (8.3.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (5.3.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (23.1)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=20 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (25.1.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (6.3.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./.venv/lib/python3.8/site-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in ./.venv/lib/python3.8/site-packages (from ipywidgets->jupyter) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in ./.venv/lib/python3.8/site-packages (from ipywidgets->jupyter) (3.0.8)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in ./.venv/lib/python3.8/site-packages (from jupyter-console->jupyter) (3.0.39)\n",
      "Requirement already satisfied: pygments in ./.venv/lib/python3.8/site-packages (from jupyter-console->jupyter) (2.15.1)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (6.8.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (0.2.2)\n",
      "Requirement already satisfied: markupsafe>=2.0 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (2.1.3)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (3.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (5.9.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in ./.venv/lib/python3.8/site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: ipython-genutils in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (0.2.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: prometheus-client in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in ./.venv/lib/python3.8/site-packages (from notebook->jupyter) (1.0.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in ./.venv/lib/python3.8/site-packages (from qtconsole->jupyter) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: zipp>=0.5 in ./.venv/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter) (3.16.1)\n",
      "Requirement already satisfied: backcall in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.2)\n",
      "Requirement already satisfied: pickleshare in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.5.0)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.8/site-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./.venv/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter) (3.8.1)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in ./.venv/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (2.7.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2.3 in ./.venv/lib/python3.8/site-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.3)\n",
      "Requirement already satisfied: fastjsonschema in ./.venv/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in ./.venv/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert->jupyter) (4.18.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.8/site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in ./.venv/lib/python3.8/site-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in ./.venv/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.4.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in ./.venv/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (6.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (2023.6.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.29.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (0.8.10)\n",
      "Requirement already satisfied: anyio>=3.1.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.7.1)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in ./.venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in ./.venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.4.4)\n",
      "Requirement already satisfied: overrides in ./.venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (7.3.1)\n",
      "Requirement already satisfied: websocket-client in ./.venv/lib/python3.8/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.6.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in ./.venv/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.8/site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.venv/lib/python3.8/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.1.2)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in ./.venv/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in ./.venv/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (6.0)\n",
      "Requirement already satisfied: rfc3339-validator in ./.venv/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in ./.venv/lib/python3.8/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.1.1)\n",
      "Requirement already satisfied: fqdn in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: isoduration in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (2.4)\n",
      "Requirement already satisfied: uri-template in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in ./.venv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.13)\n",
      "Requirement already satisfied: arrow>=0.15.0 in ./.venv/lib/python3.8/site-packages (from isoduration->jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter) (1.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imblearn in ./.venv/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in ./.venv/lib/python3.8/site-packages (from imblearn) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./.venv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.venv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in ./.venv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.venv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install wordcloud\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install tueplots\n",
    "%pip install sentence_transformers\n",
    "%pip install jupyter\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b6c42649",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Tweets\n",
    "\n",
    "The dataframe of tweets contain the following columns:\n",
    "\n",
    "- `id`: The unique identifier of the tweet\n",
    "- `text`: The content of the tweet\n",
    "- `type`: The type of tweet, which can be 'tweet', 'quoted', 'retweeted' or 'quoted__replied_to'\n",
    "- `author_id`: The unique identifier of the author of the tweet\n",
    "- `possibly_sensitive`: A boolean value indicating whether the tweet contains sensitive content\n",
    "- `retweet_count`: The number of times the tweet has been retweeted\n",
    "- `quote_count`: The number of times the tweet has been quoted\n",
    "- `reply_count`: The number of times the tweet has been replied to\n",
    "- `like_count`: The number of times the tweet has been liked\n",
    "- `followers_count`: The number of followers of the author of the tweet\n",
    "- `following_count`: The number of accounts the author of the tweet is following\n",
    "- `tweet_count`: The total number of tweets made by the author of the tweet\n",
    "- `listed_count`: The number of lists the author of the tweet is a member of\n",
    "- `score_compound`:  A numerical value ranging from -1 to 1 indicating the overall sentiment of the tweet, where -1 represents  negative sentiment and 1 represents positive sentiment. **This is the target variable for the regression task.**\n",
    "- `sentiment`: A categorical variable indicating the sentiment of the tweet, which can be 'negative', 'neutral' or 'positive'. **This is the target variable for the classification task.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762536aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:52.033832973Z",
     "start_time": "2023-06-09T15:08:52.028568092Z"
    },
    "id": "762536aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:17:16.758243: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-16 16:17:16.784434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-16 16:17:17.346863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tueplots import bundles\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "plt.rcParams.update(bundles.icml2022())\n",
    "import tueplots])\n",
    "\n",
    "X_test = min_max_scaler.transform(df_test[[\n",
    "                        'retweet_count',\n",
    "                        'quote_count',\n",
    "                        'reply_count',\n",
    ".constants.color.palettes as tue_palettes\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from torch import nn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc6b0bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:54.984833820Z",
     "start_time": "2023-06-09T15:08:54.704047946Z"
    },
    "id": "2fc6b0bb"
   },
   "outputs": [],
   "source": [
    "team_id = '30' #put your team id here\n",
    "split = 'test_1' # replace by 'test_2' for FINAL submission\n",
    "\n",
    "test_data_path = f\"https://raw.githubusercontent.com/sevaraakhbaeva/twitter-sentiment-analysis-ML23/main/data/tweets_{split}.csv\"\n",
    "train_data_path = \"https://raw.githubusercontent.com/sevaraakhbaeva/twitter-sentiment-analysis-ML23/main/data/tweets_train.csv\"\n",
    "\n",
    "df = pd.read_csv(train_data_path)\n",
    "df_test = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57cee5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:56.472262620Z",
     "start_time": "2023-06-09T15:08:56.466635703Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1682351182266,
     "user": {
      "displayName": "Batuhan Koyuncu",
      "userId": "13470316719110860762"
     },
     "user_tz": -120
    },
    "id": "57cee5aa",
    "outputId": "85c2cee9-9c1b-4b8e-daba-0f52e9c088f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>author_id</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1594715340231290880</td>\n",
       "      <td>You allege, @ylecun. that there are zero cases...</td>\n",
       "      <td>quoted</td>\n",
       "      <td>232294292</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>61</td>\n",
       "      <td>86646</td>\n",
       "      <td>5312</td>\n",
       "      <td>26291</td>\n",
       "      <td>2422</td>\n",
       "      <td>['allege', 'zero', 'cases', 'llms', 'created',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1549261742349291520</td>\n",
       "      <td>I've scanned the TOC and Introduction and I ha...</td>\n",
       "      <td>quoted</td>\n",
       "      <td>3363584909</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>65506</td>\n",
       "      <td>113</td>\n",
       "      <td>15406</td>\n",
       "      <td>856</td>\n",
       "      <td>['scanned', 'toc', 'introduction', 'strong', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1353027631130832896</td>\n",
       "      <td>Automatic differentiation is really pretty fan...</td>\n",
       "      <td>tweet</td>\n",
       "      <td>175624200</td>\n",
       "      <td>False</td>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>557</td>\n",
       "      <td>1031629</td>\n",
       "      <td>225</td>\n",
       "      <td>16316</td>\n",
       "      <td>6967</td>\n",
       "      <td>['automatic', 'differentiation', 'really', 'pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1603885102890733569</td>\n",
       "      <td>@__lucab Rienzi is especially great</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>62044012</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33022</td>\n",
       "      <td>3383</td>\n",
       "      <td>5246</td>\n",
       "      <td>359</td>\n",
       "      <td>['rienzi', 'especially', 'great']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1544232610343243776</td>\n",
       "      <td>To complete my story about the Kielce Pogrom\\n...</td>\n",
       "      <td>quoted</td>\n",
       "      <td>3363584909</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>65506</td>\n",
       "      <td>113</td>\n",
       "      <td>15406</td>\n",
       "      <td>856</td>\n",
       "      <td>['complete', 'story', 'kielce', 'pogrom', 'pic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1594715340231290880  You allege, @ylecun. that there are zero cases...   \n",
       "1  1549261742349291520  I've scanned the TOC and Introduction and I ha...   \n",
       "2  1353027631130832896  Automatic differentiation is really pretty fan...   \n",
       "3  1603885102890733569                @__lucab Rienzi is especially great   \n",
       "4  1544232610343243776  To complete my story about the Kielce Pogrom\\n...   \n",
       "\n",
       "         type   author_id  possibly_sensitive  retweet_count  quote_count  \\\n",
       "0      quoted   232294292               False              6            2   \n",
       "1      quoted  3363584909               False              3            0   \n",
       "2       tweet   175624200               False             41            5   \n",
       "3  replied_to    62044012               False              0            0   \n",
       "4      quoted  3363584909               False              0            0   \n",
       "\n",
       "   reply_count  like_count  followers_count  following_count  tweet_count  \\\n",
       "0            8          61            86646             5312        26291   \n",
       "1            1          29            65506              113        15406   \n",
       "2           13         557          1031629              225        16316   \n",
       "3            0           0            33022             3383         5246   \n",
       "4            1          14            65506              113        15406   \n",
       "\n",
       "   listed_count                                              words  \n",
       "0          2422  ['allege', 'zero', 'cases', 'llms', 'created',...  \n",
       "1           856  ['scanned', 'toc', 'introduction', 'strong', '...  \n",
       "2          6967  ['automatic', 'differentiation', 'really', 'pr...  \n",
       "3           359                  ['rienzi', 'especially', 'great']  \n",
       "4           856  ['complete', 'story', 'kielce', 'pogrom', 'pic...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Pre-process tweets\n",
    "\n",
    "The following are the preprocessing steps we followed to get the `words` column from the original tweet, which corresponds to the `text` column of the dataframe.\n",
    "\n",
    "- Remove punctuations, special characters, mentions, links, and numbers from the tweets.\n",
    "- Convert all the tweets to lowercase.\n",
    "- Tokenize the tweets into individual words.\n",
    "- Remove stop words, such as \"and\", \"the\", \"a\", etc.\n",
    "- Perform stemming or lemmatization on the remaining words to convert them to their base form.\n",
    "- Filter out any words that occur infrequently in the corpus to reduce the dimensionality of the data.\n",
    "- Create a bag of words representation of the tweets, where each tweet is represented as a vector of word frequencies.\n",
    "\n",
    "\n",
    "**Note:** Lemmatization is a process in natural language processing where words are reduced to their base form, or lemma. This is done by removing inflections, such as pluralization or verb conjugation, and converting the word to its dictionary form. The result of this process is a word that is more easily recognizable, and can be used to improve the accuracy of NLP models, such as the LDA model. By lemmatizing the words in a corpus of text, the dimensionality of the data is reduced, and the relationships between words become clearer, making it easier to identify patterns and themes within the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Make training&model selection and test split of tweets_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:08:59.473635816Z",
     "start_time": "2023-06-09T15:08:59.463195521Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df, df_inner_test = train_test_split(df, test_size=0.15, random_state=42, stratify=df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56c35dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-09T15:09:00.429554269Z",
     "start_time": "2023-06-09T15:09:00.307432330Z"
    },
    "id": "56c35dbf"
   },
   "outputs": [],
   "source": [
    "df['words_str'] = df['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_inner_test['words_str'] = df_inner_test['words'].apply(lambda words: ' '.join(eval(words)))\n",
    "df_test['words_str'] = df_test['words'].apply(lambda words: ' '.join(eval(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77914864",
   "metadata": {
    "id": "77914864"
   },
   "source": [
    "# Obtain the text embeddings\n",
    "\n",
    "Calculate embeddings and save locally.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "27ce70f9",
   "metadata": {
    "id": "27ce70f9"
   },
   "outputs": [],
   "source": [
    "# getting text-embeddings\n",
    "name = \"sentence-transformers/paraphrase-xlm-r-multilingual-v1\"\n",
    "model = SentenceTransformer(name)\n",
    "sentences = list(df.words_str.values)\n",
    "sentence_embeddings = model.encode(sentences, convert_to_numpy=True)\n",
    "sentence_embeddings_inner_test = model.encode(df_inner_test.words_str.values, convert_to_numpy=True)\n",
    "sentence_embeddings_test = model.encode(df_test.words_str.values, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "727ebf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(train, inner_test, test, name, num=True):\n",
    "\n",
    "    if not os.path.exists(f\"./{name}\"):\n",
    "        os.makedirs(f\"./{name}\")\n",
    "    if num:\n",
    "        postfix = \"num\"\n",
    "    else: \n",
    "        postfix = \"not_num\"\n",
    "\n",
    "    np.save(f\"./{name}/train_data_embeddings_{name}_n_features_{train.shape[1]}_{postfix}.npy\", train)\n",
    "    np.save(f\"./{name}/inner_test_data_embeddings_{name}_n_features_{inner_test.shape[1]}_{postfix}.npy\", inner_test)\n",
    "    np.save(f\"./{name}/test_data_embeddings_{name}_n_features_{test.shape[1]}_{postfix}.npy\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c8bf7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(\n",
    "    sentence_embeddings,\n",
    "    sentence_embeddings_inner_test,\n",
    "    sentence_embeddings_test,\n",
    "    name=\"paraphrase-xlm-r-multilingual-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c08c8",
   "metadata": {},
   "source": [
    "Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir = \"paraphrase-xlm-r-multilingual-v1\"\n",
    "for file_name in os.listdir(f\"./{dir}\"):\n",
    "    with open(f\"./{dir}/\" + file_name, \"rb\") as f:\n",
    "        if \"train\" in file_name:\n",
    "            sentence_embeddings_train = np.load(f)\n",
    "        elif \"inner_test\" in file_name:\n",
    "            sentence_embeddings_inner_test = np.load(f)\n",
    "        elif \"test\" in file_name:\n",
    "            sentence_embeddings_test = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ef9f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6800, 768)\n",
      "Inner test shape: (1200, 768)\n",
      "Test shape: (1000, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Train shape: {sentence_embeddings_train.shape}\\n\\\n",
    "Inner test shape: {sentence_embeddings_inner_test.shape}\\n\\\n",
    "Test shape: {sentence_embeddings_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b72cf",
   "metadata": {},
   "source": [
    "## Some helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b459e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode target classes\n",
    "\n",
    "def encode_target(y_text): \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_text)\n",
    "    print(f'Original classes {le.classes_}')\n",
    "    print(f'Corresponding numeric classes {le.transform(le.classes_)}')\n",
    "\n",
    "    return le.transform(y_text)\n",
    "\n",
    "# do train-validation split\n",
    "\n",
    "def train_val_split(X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    datasets = [\n",
    "        [X_train, y_train],\n",
    "        [X_val, y_val]\n",
    "    ]\n",
    "    return datasets\n",
    "\n",
    "# encode a categorical feature using one hot encoding techinique\n",
    "# categorical features = [\"type\", \"possibly_sensitive\"]\n",
    "\n",
    "def ohe(x2fit, x2transform):\n",
    "    ohe = OneHotEncoder(drop=\"if_binary\", sparse_output=False)\n",
    "    ohe.fit(x2fit)\n",
    "    x_transformed = ohe.transform(x2transform)\n",
    "    print(f'Original classes {ohe.categories_}')\n",
    "    print(f\"Original data shape: {x2transform.shape}\")\n",
    "    print(f\"Encoded data shape: {x_transformed.shape}\")\n",
    "    return x_transformed\n",
    "\n",
    "# numerical features: ['retweet_count', 'quote_count', 'reply_count', 'like_count', 'followers_count', 'following_count', 'tweet_count', 'listed_count']\n",
    "\n",
    "# predict sentiment and report metrics for train and val sets\n",
    "def predict_sentiment(datasets, model):\n",
    "    for split_name, dataset in zip(['train', 'validation'], datasets):\n",
    "        X_i, y_i = dataset\n",
    "        y_pred = model.predict(X_i)\n",
    "        print(f'\\nSplit: {split_name}')\n",
    "        print(skm.classification_report(y_i, y_pred))\n",
    "        if split_name == \"train\":\n",
    "            train_res = skm.classification_report(y_i, y_pred, output_dict=True)['macro avg']['f1-score']\n",
    "        if split_name == \"validation\":\n",
    "            val_res = skm.classification_report(y_i, y_pred, output_dict=True)['macro avg']['f1-score']\n",
    "    print(f\"Train macro av f1: {train_res}, Validation macro av f1: {val_res}\")\n",
    "\n",
    "def make_yhat_in_range(yhat):\n",
    "    \"\"\"\n",
    "    Takes predicted values of compound score and shapes them to be in range [-1,1]\n",
    "    \"\"\"\n",
    "    transform = lambda y: np.maximum(-1, np.minimum(1, y))\n",
    "    return transform(yhat)\n",
    "\n",
    "# predict score and report metrics for train and val sets\n",
    "def predict_score(datasets, model):\n",
    "    for split_name, dataset in zip(['train', 'validation'], datasets):\n",
    "        X_i, y_i = dataset\n",
    "        y_pred = model.predict(X_i)\n",
    "\n",
    "        rmse = np.sqrt(skm.mean_squared_error(y_i, y_pred))\n",
    "        print(f'\\nSplit: {split_name}')\n",
    "        print(f\"\\tRMSE: {rmse:.10f}\")\n",
    "        mae = skm.mean_absolute_error(y_i, y_pred)\n",
    "        print(f\"\\tMAE: {mae:.10f}\")\n",
    "\n",
    "        if split_name == \"train\":\n",
    "            train_res = round(rmse,10)\n",
    "        if split_name == \"validation\":\n",
    "            val_res = round(rmse,10)\n",
    "\n",
    "    print(f\"Train rmse: {train_res}, Validation rmse: {val_res}\")\n",
    "    return (train_res, val_res)\n",
    "\n",
    "# predict and report sentiment for test set\n",
    "def predict_sentiment_test(X_test, y_test, model):\n",
    "    print('\\nSplit: test')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(skm.classification_report(y_test, y_pred))\n",
    "    print(f\"Test macro av f1:{skm.classification_report(y_test, y_pred, output_dict=True)['macro avg']['f1-score']}\")\n",
    "\n",
    "def predict_score_test(X_test, y_test, model):\n",
    "    print('\\nSplit: test')\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(skm.mean_squared_error(y_test, y_pred))\n",
    "    print(f\"\\tRMSE: {rmse:.10f}\")\n",
    "    mae = skm.mean_absolute_error(y_test, y_pred)\n",
    "    print(f\"\\tMAE: {mae:.10f}\")\n",
    "\n",
    "\n",
    "#def predict_sentiment2submit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d83b11",
   "metadata": {},
   "source": [
    "## Try\n",
    "### Classification\n",
    "(1) Embeddings paraphrase-xlm-r-multilingual-v1 + lda + svm with costs + add all other num features without preprocessing\n",
    "\n",
    "With \"balanced\" cost very bad at predicting 0 ie negative class. Try to adjust cost matrix.\n",
    "\n",
    "Increasing gamma improves minor class detection but severes overfitting -- macro average on train and val differ ~ 0.09-0.20\n",
    "\n",
    "**C = 2, gamma = 0.1**  \n",
    "Train macro av f1: 0.8964651333383055, Validation macro av f1: 0.5871189058432217\n",
    "\n",
    "**[best, submit] C = 2, gamma = 0.01**  \n",
    "Train macro av f1: 0.8066099716513456, Validation macro av f1: 0.6239099469103556\n",
    "\n",
    "**C = 0.1, gamma = 0.01**  \n",
    "Train macro av f1: 0.527141231712822, Validation macro av f1: 0.501978441806522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2ce57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes ['negative' 'neutral' 'positive']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "Original classes ['negative' 'neutral' 'positive']\n",
      "Corresponding numeric classes [0 1 2]\n",
      "Original classes [array(['quoted', 'quoted__replied_to', 'replied_to', 'retweeted', 'tweet'],\n",
      "      dtype=object), array([False])]\n",
      "Original data shape: (6800, 2)\n",
      "Encoded data shape: (6800, 6)\n",
      "Original classes [array(['quoted', 'quoted__replied_to', 'replied_to', 'retweeted', 'tweet'],\n",
      "      dtype=object), array([False])]\n",
      "Original data shape: (1200, 2)\n",
      "Encoded data shape: (1200, 6)\n"
     ]
    }
   ],
   "source": [
    "n_components = 2\n",
    "solver = \"svd\"\n",
    "lda = LinearDiscriminantAnalysis(n_components=n_components, solver=solver)\n",
    "\n",
    "\n",
    "X_embed_transformed_train = lda.fit_transform(sentence_embeddings_train, encode_target(df.sentiment))\n",
    "X_embed_transformed_inner_test = lda.transform(sentence_embeddings_inner_test)\n",
    "\n",
    "y = encode_target(df.sentiment)\n",
    "\n",
    "X_cat_encoded_train = ohe(x2fit=df[[\"type\", \"possibly_sensitive\"]], x2transform=df[[\"type\", \"possibly_sensitive\"]])\n",
    "X_cat_encoded_inner_test = ohe(x2fit=df[[\"type\", \"possibly_sensitive\"]], x2transform=df_inner_test[[\"type\", \"possibly_sensitive\"]])\n",
    "\n",
    "kernel = \"rbf\"\n",
    "C = 2\n",
    "gamma = 0.01\n",
    "class_weight = \"balanced\" # {0: 1000, 1: 1, 2: 3}\n",
    "\n",
    "svm_classifier = SVC(kernel=kernel, C=C, gamma=gamma, class_weight=class_weight, random_state=42)\n",
    "\n",
    "X = np.concatenate((X_embed_transformed_train, X_cat_encoded_train,\n",
    "                    df[[\n",
    "                        'retweet_count',\n",
    "                        'quote_count',\n",
    "                        'reply_count',\n",
    "                        'like_count',\n",
    "                        'followers_count',\n",
    "                        'following_count',\n",
    "                        'tweet_count',\n",
    "                        'listed_count',\n",
    "                        ]]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67448137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split: train\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.95      0.68       331\n",
      "           1       0.96      0.84      0.90      3576\n",
      "           2       0.81      0.89      0.85      1533\n",
      "\n",
      "    accuracy                           0.86      5440\n",
      "   macro avg       0.76      0.89      0.81      5440\n",
      "weighted avg       0.89      0.86      0.87      5440\n",
      "\n",
      "\n",
      "Split: validation\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.70      0.51        79\n",
      "           1       0.80      0.81      0.80       920\n",
      "           2       0.61      0.50      0.55       361\n",
      "\n",
      "    accuracy                           0.72      1360\n",
      "   macro avg       0.61      0.67      0.62      1360\n",
      "weighted avg       0.73      0.72      0.72      1360\n",
      "\n",
      "Train macro av f1: 0.8066099716513456, Validation macro av f1: 0.6239099469103556\n"
     ]
    }
   ],
   "source": [
    "datasets = train_val_split(X, y)\n",
    "svm_classifier.fit(datasets[0][0], datasets[0][1])\n",
    "predict_sentiment(datasets=datasets, model=svm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c07a377",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "(1) Lasso feature selection on all features (embeddings + everything else) + kernel SVM + one hot encoding for categorical features + min max scale for \"retweet_count\", \"quote_count\", \"reply_count\", \"like_count\", \"followers_count\", \"following_count\", \"tweet_count\", \"listed_count\"\n",
    "\n",
    "**No feature selection, C = 2, gamma = 0.001**  \n",
    "Train rmse: 0.3091831285, Validation rmse: 0.3310825095\n",
    "\n",
    "**Lasso feature selection, C = 2, gamma = 0.001**  \n",
    "Train rmse: 0.3194176843, Validation rmse: 0.3284631055\n",
    "\n",
    "**[best, submit] Lasso feature selection, C = 2, gamma = 0.01**  \n",
    "*starts to overfit*  \n",
    "Train rmse: 0.2741596674, Validation rmse: 0.3171751508\n",
    "\n",
    "-> exclude correlated features \"quote_count\", \"like_count\", \"listed_count\" \n",
    "Train rmse: 0.2749838576, Validation rmse: 0.3173034045\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed06c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes [array(['quoted', 'quoted__replied_to', 'replied_to', 'retweeted', 'tweet'],\n",
      "      dtype=object), array([False])]\n",
      "Original data shape: (6800, 2)\n",
      "Encoded data shape: (6800, 6)\n",
      "Original classes [array(['quoted', 'quoted__replied_to', 'replied_to', 'retweeted', 'tweet'],\n",
      "      dtype=object), array([False])]\n",
      "Original data shape: (1200, 2)\n",
      "Encoded data shape: (1200, 6)\n",
      "Original classes [array(['quoted', 'quoted__replied_to', 'replied_to', 'retweeted', 'tweet'],\n",
      "      dtype=object), array([False])]\n",
      "Original data shape: (1000, 2)\n",
      "Encoded data shape: (1000, 6)\n",
      "Num of fetures initially: 782\n",
      "Num of features selected: 221\n"
     ]
    }
   ],
   "source": [
    "# scale numerical features\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_num_scaled = min_max_scaler.fit_transform(df[[\n",
    "                        'retweet_count',\n",
    "                        'quote_count',\n",
    "                        'reply_count',\n",
    "                        'like_count',\n",
    "                        'followers_count',\n",
    "                        'following_count',\n",
    "                        'tweet_count',\n",
    "                        'listed_count',\n",
    "                        ]])\n",
    "X_num_scaled_inner_test = min_max_scaler.transform(df_inner_test[[\n",
    "                        'retweet_count',\n",
    "                        'quote_count',\n",
    "                        'reply_count',\n",
    "                        'like_count',\n",
    "                        'followers_count',\n",
    "                        'following_count',\n",
    "                        'tweet_count',\n",
    "                        'listed_count',\n",
    "]])\n",
    "\n",
    "X_test = min_max_scaler.transform(df_test[[\n",
    "                        'retweet_count',\n",
    "                        'quote_count',\n",
    "                        'reply_count',\n",
    "                        'like_count',\n",
    "                        'followers_count',\n",
    "                        'following_count',\n",
    "                        'tweet_count',\n",
    "                        'listed_count',\n",
    "]])\n",
    "\n",
    "# encode categorical\n",
    "X_cat_encoded_train = ohe(x2fit=df[[\"type\", \"possibly_sensitive\"]], x2transform=df[[\"type\", \"possibly_sensitive\"]])\n",
    "X_cat_encoded_inner_test = ohe(x2fit=df[[\"type\", \"possibly_sensitive\"]], x2transform=df_inner_test[[\"type\", \"possibly_sensitive\"]])\n",
    "X_cat_encoded_test = ohe(x2fit=df[[\"type\", \"possibly_sensitive\"]], x2transform=df_test[[\"type\", \"possibly_sensitive\"]])\n",
    "\n",
    "X = np.concatenate((sentence_embeddings_train, X_cat_encoded_train, X_num_scaled), axis=1)\n",
    "X_inner_test = np.concatenate((sentence_embeddings_inner_test, X_cat_encoded_inner_test, X_num_scaled_inner_test), axis=1)\n",
    "\n",
    "# do feature selection\n",
    "alpha = 0.001\n",
    "fit_intercept = False\n",
    "max_iter = 10000\n",
    "\n",
    "lasso = Lasso(alpha=alpha, fit_intercept=fit_intercept, random_state=42, max_iter=max_iter)\n",
    "lasso.fit(X, df.score_compound.values)\n",
    "\n",
    "print(f\"Num of fetures initially: {X.shape[1]}\")\n",
    "\n",
    "X = X[:,lasso.coef_!=0]\n",
    "X_inner_test = X_inner_test[:,lasso.coef_!=0]\n",
    "y = df.score_compound.values\n",
    "y_inner_test = df_inner_test.score_compound.values\n",
    "\n",
    "print(f\"Num of features selected: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eb02af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split: train\n",
      "\tRMSE: 0.2741596674\n",
      "\tMAE: 0.2017250976\n",
      "\n",
      "Split: validation\n",
      "\tRMSE: 0.3171751508\n",
      "\tMAE: 0.2428035400\n",
      "Train rmse: 0.2741596674, Validation rmse: 0.3171751508\n",
      "\n",
      "Split: test\n",
      "\tRMSE: 0.3253445767\n",
      "\tMAE: 0.2510602187\n"
     ]
    }
   ],
   "source": [
    "kernel = \"rbf\"\n",
    "C = 2\n",
    "gamma = 0.01\n",
    "epsilon = 0.1\n",
    "shrinking = True\n",
    "svr = SVR(kernel=kernel,\n",
    "                     C=C, gamma=gamma,\n",
    "                     epsilon = epsilon,\n",
    "                     shrinking = shrinking)\n",
    "\n",
    "datasets = train_val_split(X, y)\n",
    "svr.fit(datasets[0][0], datasets[0][1])\n",
    "predict_score(datasets=datasets, model=svr)\n",
    "predict_score_test(X_inner_test, y_inner_test, svr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
